# -*- coding: utf-8 -*-
"""
Created on Tue Nov 16 21:01:08 2021

@author: tkdnj
"""

# -*- coding: utf-8 -*-
"""007. wide-deep-RecSys model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OAwdaHJJvRpRIpkjOzx6EXo6N3QPQL4H

# 블로그 설명

해당 자료에 대한 설명은 아래 블로그에 올려두었습니다.
- https://lsjsj92.tistory.com/597
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures, LabelEncoder
from sklearn.model_selection import StratifiedKFold
from tensorflow.keras.layers import Input, Embedding, Dense, Flatten, Dropout, SpatialDropout1D, Activation, concatenate
from tensorflow.keras.optimizers import Adam, SGD
from tensorflow.python.keras.layers.advanced_activations import ReLU, PReLU, LeakyReLU, ELU
from tensorflow.python.keras.layers.normalization import BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.models import Model
from tensorflow.keras.utils import plot_model
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import load_model
from tensorflow import keras
import os
import random
os.chdir(r'F:\2021\study\5_화물배차\wide deep code\유림수정')

raw_data=pd.read_csv('cargolist.cargo_full.csv',encoding='CP949')


"""데이터 전처리"""
data_group = raw_data.loc[raw_data['intStatus']==3,]
# data_group = data_group.loc[(data_group['intDistance']<=600)&(data_group['intDistance']>=0),]
# data_group = data_group.loc[-((data_group['intDistance']==0)&(data_group['dwUpPoiX']!=data_group['dwDwPoiX'])),]
data_group = data_group.loc[data_group['intMemberPricecode']!=0,]
data_group = data_group.loc[data_group['dateUpcode']!=0,]
data_group = data_group.loc[data_group['dateDowncode']!=0,]
data_group = data_group.loc[data_group['strUpZonecode']!=0,]
data_group = data_group.loc[data_group['strDwZonecode']!=0,]
data_group = data_group.loc[data_group['intCarTypecode2']!=0,]
data_group = data_group.loc[data_group['intCarSizecode2']!=0,]
data_group = data_group.loc[data_group['intDistancecode']!=0,]


data_group['Group'] = 0
group=1
for type in range(1,5):
    for size in range(1,4):
        data_group.loc[(data_group['intCarTypecode2']==type)&(data_group['intCarSizecode2']==size),'Group'] = group
        group+=1



COLUMNS = data_group.columns

CONTINUOUS_COLUMNS=['intDistance', 'intMemberPrice','dateUpint', 'dateDownint']
CATEGORICAL_COLUMNS=['strUpZone', 'strDwZone']
#CATEGORICAL_COLUMNS=['intUpDateType2', 'intDwDateType2','strUpZone', 'strDwZone','strJAddress','strBirth']


"""## 라벨 값(y) 생성"""
### 그룹별 for문
#for group_num in range(1,group):
#data = data_group.loc[data_group['Group']==group_num,]
#for group_num in range(1,group):
group_num=10
data_group_1 = data_group.loc[data_group['Group']==group_num,].reset_index(drop=True)
member100=data_group_1['strTrsNum'].value_counts()
data = data_group_1.loc[data_group_1['strTrsNum'].isin(member100[member100>=3].index)].reset_index(drop=True)
 


le = LabelEncoder()
data['strTrsNum'] = le.fit_transform(data['strTrsNum'])
data['label'] = data['strTrsNum']

data.drop('strTrsNum', axis = 1, inplace=True)

data.head()

y = data['label'].values
data.drop('label', axis = 1, inplace=True)
catenum=len(np.unique(y))
#data.head()

#y2 = [1 if i==9201 else 0 for i in y]

"""## 카테고리 값들을 숫자로 변경"""

for c in CATEGORICAL_COLUMNS:
    le = LabelEncoder()
    data[c] = le.fit_transform(data[c])

data.head()

"""## 정규화"""
scaler = MinMaxScaler()
data[CONTINUOUS_COLUMNS] = scaler.fit_transform(data[CONTINUOUS_COLUMNS])
#X_train_countinue = scaler.fit_transform(X_train_countinue)
#X_test_countinue = scaler.transform(X_test_countinue)

    
"""# 다시 trian, test로 구분 """
#X_train, X_test, y_train, y_test = train_test_split(data, y, test_size = 0.2, random_state=1)
X_train, X_test, y_train2, y_test2 = train_test_split(data, y, test_size = 0.2, stratify=y, random_state=1) #stratify : label 비율 맞춰서 sampling

X_test=X_test.reset_index(drop=True)
X_train=X_train.reset_index(drop=True)

"""
train_size = len(train_data)

X_train = data.iloc[:train_size]
y_train = y[:train_size]
X_test = data.iloc[train_size:]
y_test = y[train_size:]
"""
print(len(X_train), len(y_train2), len(X_test), len(y_test2))

"""## 모델 설계"""
        
def get_deep_model():

    category_inputs = []
    category_embeds = []
    # 카테고리컬 데이터 임베딩
    for i in range(len(CATEGORICAL_COLUMNS)):
        input_i = Input(shape=(1,), dtype='int32')
        dim = 20#len(np.unique(data[CATEGORICAL_COLUMNS[i]]))
        embed_dim = int(np.ceil(dim ** 0.5)) # embedding 차원을 0.5배 정도로 해서 한다.
        embed_i = Embedding(dim, embed_dim, input_length=1)(input_i)
        flatten_i = Flatten()(embed_i)
        category_inputs.append(input_i)
        category_embeds.append(flatten_i)
    # continuous 데이터 input
    continue_input = Input(shape=(len(CONTINUOUS_COLUMNS),))
    continue_dense = Dense(256, use_bias=False)(continue_input)
    # category와 continue를 합침
    concat_embeds = concatenate([continue_dense] + category_embeds)
    concat_embeds = Activation('relu')(concat_embeds)
    bn_concat = BatchNormalization()(concat_embeds)

    fc1 = Dense(512, use_bias=False)(bn_concat)
    relu1 = ReLU()(fc1)
    bn1 = BatchNormalization()(relu1)
    fc2 = Dense(256, use_bias=False)(bn1)
    relu2 = ReLU()(fc2)
    bn2 = BatchNormalization()(relu2)
    fc3 = Dense(128)(bn2)
    relu3 = ReLU()(fc3)
    
    return category_inputs, continue_input, relu3

def get_wide_model():
    dim = X_train_category_poly.shape[1]
    print("########",dim)
    return Input(shape=(dim,))





#########################################

for person_num in range(0,catenum):
    """## 사람마다 0,1로 변형, 0개수 sampling(catenum>2)"""
    #y_train_mod0 = np.where(y_train2==person_num,1,0)
    y_training=keras.utils.to_categorical(y_train2)

    """## K-Fold 시작"""
    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)
    cv_accuracy = []
    n_iter = 0
    #WD_idx=[]
    for tr_idx, val_idx in skf.split(X_train, y_train2):  # feautres 데이터를 위에서 지정한 kfold 숫자로 분할
        X_training, X_val = X_train.loc[tr_idx,], X_train.loc[val_idx,]
        y_train, y_val = y_training[tr_idx], y_training[val_idx]

        """## 카테고리 값들과 연속값들을 뽑아냄"""
        
        X_train_category = np.array(X_training[CATEGORICAL_COLUMNS])
        X_val_category = np.array(X_val[CATEGORICAL_COLUMNS])
        X_train_countinue = np.array(X_training[CONTINUOUS_COLUMNS], dtype='float64')
        X_val_countinue = np.array(X_val[CONTINUOUS_COLUMNS], dtype='float64')
        
        
        """## Polynomial 하게 바꿔줌 (비선형적인 설정으로 선형 회귀를 확장하는 방법. 즉 다항식 함수로 바꿔줌)
        
        카테고리 값을 Polynomial로 바꿔준다.
        """
        
        poly = PolynomialFeatures(degree=2, interaction_only=True)
        X_train_category_poly = poly.fit_transform(X_train_category)
        X_val_category_poly = poly.transform(X_val_category)
    
    
        """## (모델 생성) wide 모델과 deep model을 합쳐준다. """
        category_inputs, continue_input, deep_model = get_deep_model()
        wide_model = get_wide_model()

        out_layer = concatenate([deep_model, wide_model])
        inputs = [continue_input] + category_inputs + [wide_model]
        output = Dense(catenum, activation='softmax')(out_layer)
        #output = Dense(1, activation='softmax')(out_layer)
        model = Model(inputs=inputs, outputs=output)
        
        model.summary()
        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
        
        #######################################
        """## 입력 데이터
        
        - conti 데이터 -> category 데이터 -> poly data 순으로 입력으로 넣어준다.
        
        """
        
        input_data = [X_train_countinue] + [X_train_category[:, i] for i in range(X_train_category.shape[1])] + [X_train_category_poly]
        
        epochs = 10
        batch_size = 128
        
        checkpoint_path = '1222wide-deep best model_{}_{}_{}.h5'.format(group_num, person_num, n_iter)
        checkpoint = ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True) #best model save
        early_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=100) #val_loss가 최소일 떄 earlystopping
        #model.fit(input_data, y_train, epochs=epochs, batch_size=batch_size, validation_data=(eval_input_data,y_test), callbacks=[early_stopping,checkpoint])
        model.fit(input_data, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2, callbacks=[early_stopping,checkpoint])
        #validation_split은 데이터 뒤 20%를 validation으로 사용하는 것임
           
        ###########################
        """# predict"""
        eval_input_data = [X_val_countinue] + [X_val_category[:, i] for i in range(X_val_category.shape[1])] + [X_val_category_poly]
    
        model = load_model('1222wide-deep best model_{}_{}_{}.h5'.format(group_num, person_num, n_iter))
        
        val_pred=model.predict(eval_input_data, batch_size=batch_size)
        
        count=0
        for prednum in range(len(val_pred)):
            prediction10 = sorted(range(len(val_pred[prednum])),key= lambda i: val_pred[prednum][i])[-10:]
            if np.where(y_val==1)[1][prednum] in prediction10:
                count+=1
        
        cv_accuracy.append(np.round((count/len(val_pred)*100), 5))
        n_iter+=1
        
    
    """## Member test accuracy 계산"""
    y_training=keras.utils.to_categorical(y_test2)
    X_test_category = np.array(X_test[CATEGORICAL_COLUMNS])
    X_test_countinue = np.array(X_test[CONTINUOUS_COLUMNS], dtype='float64')
    poly = PolynomialFeatures(degree=2, interaction_only=True)
    X_test_category_poly = poly.fit_transform(X_test_category)
    test_input_data = [X_test_countinue] + [X_test_category[:, i] for i in range(X_test_category.shape[1])] + [X_test_category_poly]
    
    best_iter = cv_accuracy.index(max(cv_accuracy))
    model = load_model('1222wide-deep best model_{}_{}_{}.h5'.format(group_num, person_num, best_iter))

    pred=model.predict(test_input_data, batch_size=batch_size)

    globals()['member_count_{}_{}'.format(group_num, person_num)]=0
    for prednum in range(len(pred)):
        prediction10 = sorted(range(len(pred[prednum])),key= lambda i: pred[prednum][i])[-10:]
        if y_test2[prednum] in prediction10:
            globals()['member_count_{}_{}'.format(group_num, person_num)]+=1
    print("===== Member ",person_num," Done! =====")





"""## test 정확도 member별로 print """
for i in range(len(person_num)):
    print(np.round((globals()['member_count_{}_{}'.format(group_num, person_num)]/len(pred)*100), 5))
    
    
# =============================================================================
# ################### 예측력 print
# for group_num in range(1,group):
#     data_group_1 = data_group.loc[data_group['Group']==group_num,].reset_index(drop=True)
#     member100=data_group_1['strTrsNum'].value_counts()
#     data = data_group_1.loc[data_group_1['strTrsNum'].isin(member100[member100>=100].index)].reset_index(drop=True)
#          
#     le = LabelEncoder()
#     data['strTrsNum'] = le.fit_transform(data['strTrsNum'])
#     data['label'] = data['strTrsNum']
#     
#     y = data['label'].values
#     catenum=len(np.unique(y))
#     
#     for person_num in range(0,catenum):
#         print('model_{}group_{}person : '.format(group_num, person_num),len(y_test),"개 중 ",globals()['count_{}_{}'.format(group_num, person_num)],"개 맞춤",globals()['count_{}_{}'.format(group_num, person_num)]/len(y_test)*100,"%")
# 
# =============================================================================
    